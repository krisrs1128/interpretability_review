---
title: "Directly Interpretable Models"
output: html_document
date: "`r Sys.Date()`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

```{r}
library(tidyverse)
theme_set(theme_classic())
set.seed(20240201)
source("estimate/interpretable.R")
```

```{r}
samples <- read_csv("data/blooms.csv") |>
  mutate(class = factor(class))
xy <- select(samples, -subject)
p <- list()
```

```{r}
problem_rec <- recipe(class ~ ., data = xy) |>
  update_role(class, new_role = "outcome") |>
  step_normalize(all_numeric())

wf <- workflow() |>
  add_recipe(problem_rec)

tune_spec <- logistic_reg(penalty = tune(), mixture = 1) |>
  set_engine("glmnet")

lasso_result <- lasso_outputs(tune_spec, wf, xy)
p[["lasso_raw"]] <- lasso_result$plot +
  labs(title = "(a) Original Lasso")
lasso_result
```

```{r}
p[["stability_raw"]] <- compare_splits(xy, lasso_result$grid)$p +
  theme(legend.position = "none") +
  labs(title = "(e) Original Lasso Stability")
```

Here is the analogous code for a decision tree.

```{r}
tree_spec <- decision_tree(cost_complexity = tune()) |>
  set_engine("rpart") |>
  set_mode("classification")

final_tree <- tree_outputs(tree_spec, wf, xy)
p[["tree_raw"]] <- final_tree$plot +
  labs(title = "(c) Original Tree")
final_tree
```

Next approach to directly interpretability is to manually define predictive
features.

```{r}
samples_long <- samples |>
  pivot_longer(starts_with("tax"), names_to = "feature", values_to = "Freq") |>
  mutate(
    taxon = str_extract(feature, "tax[0-9]+"),
    time = as.integer(str_extract(feature, "[0-9]+$"))
  )

features <- samples_long |>
  group_by(taxon, subject) |>
  summarise(
    slope = coef(fastLm(Freq ~ time))[1],
    curvature = mean(diff(Freq, 2)^2)
  )

xy_features <- samples_long |>
  select(subject, class, taxon) |>
  unique() |>
  left_join(features) |>
  pivot_wider(names_from = "taxon", values_from = c("slope", "curvature")) |>
  select(-subject)
```

```{r}
problem_rec <- recipe(class ~ ., data = xy_features) |>
  update_role(class, new_role = "outcome") |>
  step_normalize(all_numeric())

wf <- workflow() |>
  add_recipe(problem_rec)

tune_spec <- logistic_reg(penalty = tune(), mixture = 1) |>
  set_engine("glmnet")
tree_spec <- decision_tree(cost_complexity = tune()) |>
  set_engine("rpart") |>
  set_mode("classification")

lasso_result <- lasso_outputs(tune_spec, wf, xy_features)
lasso_result
p[["lasso_features"]] <- lasso_result$p +
  labs(title = "(b) Featurized Lasso") +
  scale_y_discrete(
    labels = \(x) str_replace(x, "curvature", "c") |> str_replace("slope", "s")
  ) +
  theme(axis.text.y = element_text(size = 5))

tree_result <- tree_outputs(tree_spec, wf, xy_features)
p[["tree_features"]] <- tree_result$plot +
  labs(title = "(d) Featurized Tree")

tree_result
```

```{r}
comparison <- compare_splits(xy_features, lasso_result$grid)

p[["stability_features"]] <- comparison$plot +
  labs(title = "(f) Featurized Lasso Stability")
```

```{r}
(p[["lasso_raw"]] / p[["lasso_features"]]) |
(p[["tree_raw"]] / p[["tree_features"]]) |
(p[["stability_raw"]] / p[["stability_features"]]) +
  plot_layout(guides = "collect")
ggsave("data/direct_interpretability_simulation.png", width = 16, height = 8)
```

```{r}
comparison$coef |>
  filter(abs(`1`) > 0 & abs(`2`) > 0)
```