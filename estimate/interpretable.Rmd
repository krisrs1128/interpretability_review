---
title: "Directly Interpretable Models"
output: rmdformats::readthedown
css: custom.css
date: "`r Sys.Date()`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

The main estimation functions are defined in `interpretable.R`, sourced below.

```{r}
library(tidyverse)
theme_set(theme_classic())
set.seed(20240201)
source("estimate/interpretable.R")
subsample <- 1000
```

## Original Data

Our first approach is to run classifiers on the original microbiome trajectory
data (`blooms.csv`) without any processing or featurization.  Let's read in the
data, ensure that the disease/health class is a factor, and create a list `p`
within which to store all our visualiations.

```{r}
samples <- read_csv("data/blooms.csv") |>
  mutate(class = factor(class)) |>
  top_n(subsample)
xy <- select(samples, -subject)
p <- list()
```

### Direct Sparse Logistic Regression

We are using the `tidymodels` package to ensure consistency across our lasso,
tree, and featurization workflows. The `wf` object species the steps to be taken
before regression (the `recipe`). Within `lasso_outputs`, we run four-fold
cross-validation with a grid of 50 $\lambda$ values.

```{r}
problem_rec <- recipe(class ~ ., data = xy) |>
  update_role(class, new_role = "outcome") |>
  step_normalize(all_numeric())

wf <- workflow() |>
  add_recipe(problem_rec)

tune_spec <- logistic_reg(penalty = tune(), mixture = 1) |>
  set_engine("glmnet")

lasso_result <- lasso_outputs(tune_spec, wf, xy)
p[["lasso_raw"]] <- lasso_result$plot +
  labs(title = "(a) Original Lasso")
lasso_result
errors(lasso_result)
```

The block below runs the stability analysis. Internally, `compare_splits` splits
the data in half, tunes a lasso model on each split, and returns both the figure
(`p`) and the original coefficients for each model.

```{r}
p[["stability_raw"]] <- compare_splits(xy, lasso_result$grid)$p +
  theme(legend.position = "none") +
  labs(title = "(e) Original Lasso Stability")
```

```{r}
```

### Direct Decision Tree

Thanks to tidymodels, the decision tree code is almost exactly the same.

```{r}
tree_spec <- decision_tree(cost_complexity = tune()) |>
  set_engine("rpart") |>
  set_mode("classification")

final_tree <- tree_outputs(tree_spec, wf, xy)
p[["tree_raw"]] <- final_tree$plot +
  labs(title = "(c) Original Tree")
final_tree
errors(final_tree)
```

## Featurization

Our second approach defines trend and curvature features. To do this, we first
reshape the data into long format (`pivot_longer`) and then iterate over each
trajectory to compute the summaries (`group_by` + `summarise`). For prediction,
we still need each summary to be laid out along columns, and this is the reason
for the final `pivot_wider` block.

```{r}
samples_long <- samples |>
  pivot_longer(starts_with("tax"), names_to = "feature", values_to = "Freq") |>
  mutate(
    taxon = str_extract(feature, "tax[0-9]+"),
    time = as.integer(str_extract(feature, "[0-9]+$"))
  )

features <- samples_long |>
  group_by(taxon, subject) |>
  summarise(
    slope = coef(fastLm(Freq ~ time))[1],
    curvature = mean(diff(Freq, 2)^2)
  )

xy_features <- samples_long |>
  select(subject, class, taxon) |>
  unique() |>
  left_join(features) |>
  pivot_wider(names_from = "taxon", values_from = c("slope", "curvature")) |>
  select(-subject)
```

From here, the estimation code is almost the same as before. The block below
runs both the lasso and the decision tree models.

```{r}
problem_rec <- recipe(class ~ ., data = xy_features) |>
  update_role(class, new_role = "outcome") |>
  step_normalize(all_numeric())

wf <- workflow() |>
  add_recipe(problem_rec)

tune_spec <- logistic_reg(penalty = tune(), mixture = 1) |>
  set_engine("glmnet")
tree_spec <- decision_tree(cost_complexity = tune()) |>
  set_engine("rpart") |>
  set_mode("classification")

lasso_result <- lasso_outputs(tune_spec, wf, xy_features)
tree_result <- tree_outputs(tree_spec, wf, xy_features)
```

The model results are printed below, and we continue to save figures into the
list `p`. The feature names are now quite long, so we're using `str_replace` in
a (mostly futile) effort to make them more readable.

```{r}
lasso_result
errors(lasso_result)
p[["lasso_features"]] <- lasso_result$p +
  labs(title = "(b) Featurized Lasso") +
  scale_y_discrete(
    labels = \(x) str_replace(x, "curvature", "c") |> str_replace("slope", "s")
  ) +
  theme(axis.text.y = element_text(size = 5))

tree_result
errors(final_tree)
p[["tree_features"]] <- tree_result$plot +
  labs(title = "(d) Featurized Tree")
```

This reruns the stability analysis on the new features. If you look into the
`comparison` object, you can count the number of coefficients that are nonzero
in both splits.

```{r}
comparison <- compare_splits(xy_features, lasso_result$grid)
p[["stability_features"]] <- comparison$plot +
  labs(title = "(f) Featurized Lasso Stability")
```

Finally, we can combine all the subfigures that we've been accumulating into `p`
to create the final version of Figure 1 in the text.

```{r}
(p[["lasso_raw"]] / p[["lasso_features"]]) |
  (p[["tree_raw"]] / p[["tree_features"]]) |
  (p[["stability_raw"]] / p[["stability_features"]]) +
    plot_layout(guides = "collect")
ggsave(glue("interpretable-{n_sample}.png"))
```