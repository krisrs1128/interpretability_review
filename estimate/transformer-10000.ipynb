{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer on Simulated Trajectories\n",
    "\n",
    "Before we can estimate any model, we need to reshape the data so that we can sample random subjects in each batch. This is accomplished using the `LinearData` loader, defined in the accompanying `transformer.py` script. We have reserved 375 samples for training and 125 for validation. You can download the data from [this link](https://github.com/krisrs1128/interpretability_review/tree/main/data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from transformer import LinearData\n",
    "from transformer import Transformer\n",
    "\n",
    "torch.manual_seed(20240210)\n",
    "subsample_size = 10000\n",
    "samples_df = pd.read_csv(\"../data/blooms.csv\", nrows=subsample_size)\n",
    "\n",
    "dataset = LinearData(samples_df)\n",
    "train = Subset(dataset, torch.arange(int(0.75 * subsample_size)))\n",
    "validation = Subset(dataset, torch.arange(int(0.75 * subsample_size), subsample_size))\n",
    "loaders = {\n",
    "  \"train\": DataLoader(train, batch_size=16),\n",
    "  \"validate\": DataLoader(validation, batch_size=16)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we let's define a model with a forward function that lets us get predicted probabilities for the two classes given the historical microbiome profile so far. Just to make sure that this works as expected, let's pass in some random data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer()\n",
    "z, probs = model(torch.randn((16, 50, 144)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train the model based on the input data loader, using a lightning trainer. Training and validation accuracies can be checked by starting a tensorboard viewer in the `lightning_logs` directory (i.e., `tensorboard --logdir=path/to/lightning_logs`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "from transformer import LitTransformer\n",
    "\n",
    "lit_model = LitTransformer(model)\n",
    "trainer = L.Trainer(max_epochs=70)\n",
    "\n",
    "%time trainer.fit(lit_model, loaders[\"train\"], loaders[\"validate\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case we're interested, we can extract predicted probabilities for each sample. We set the model to evaluation mode and iterate over each sample in both the training and validation loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_model.model.eval()\n",
    "p_hat = []\n",
    "with torch.no_grad():\n",
    "  for x, _ in loaders[\"train\"]:\n",
    "    p_hat.append(lit_model.model(x)[1])\n",
    "\n",
    "  for x, _ in loaders[\"validate\"]:\n",
    "    p_hat.append(lit_model.model(x)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interpretability",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
