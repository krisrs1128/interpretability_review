---
title: "Directly Interpretable Models"
output: html_document
date: "`r Sys.Date()`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

```{r}
library(RcppEigen)
library(broom)
library(ggrepel)
library(glmnetUtils)
library(glue)
library(patchwork)
library(rpart.plot)
library(tidymodels)
library(tidyverse)
my_theme <- theme_classic() +
  theme()
theme_set(my_theme)
set.seed(20240201)
```

```{r}
error_rate <- function(spec, xy) {
  y_hat <- fit(spec, class ~ ., data = xy) |>
    predict(xy)
  errors <- bind_cols(class = xy$class, y_hat) |>
    count(class, .pred_class)
  list(errors = errors, fit = fit)
}

#' Wrapper to run the tidymodels glmnet workflow
lasso_outputs <- function(tune_spec, xy) {
  control <- control_grid(verbose = TRUE, save_workflow = TRUE)
  lasso_grid <- tune_grid(
    wf |> add_model(tune_spec),
    vfold_cv(xy, v = 4),
    grid = grid_regular(penalty(range=c(-2.66610, -0.66610)), levels = 50),
    control = control
  )

  p1 <- collect_metrics(lasso_grid) |>
    filter(.metric == "accuracy") |>
    ggplot(aes(log(penalty))) +
    geom_errorbar(aes(
      ymin = mean - std_err,
      ymax = mean + std_err
    ),
    alpha = 0.5
    ) +
    geom_point(aes(y = mean), col = "#545454") +
    scale_x_reverse() +
    labs(x = expression(log(lambda)), y = "CV Accuracy") +
    facet_wrap(~ .metric)

  final_lasso <- fit_best(lasso_grid, "accuracy")
  coefs <- tidy(final_lasso$fit$fit$fit)
  p2 <- ggplot(coefs) +
    geom_hline(yintercept = 0, linewidth = 1.5) +
    geom_line(aes(log(lambda), estimate, group = term), col = "#0c0c0c") +
    labs(x = expression(log(lambda)), y = "Coefficient Estimate") +
    scale_x_reverse()

  list(plot = p1 / p2, fit = final_lasso, grid = lasso_grid)
}

#' Wrapper to run the tidymodels rpart workflow
tree_outputs <- function(tree_spec, xy) {
  control <- control_grid(verbose = TRUE, save_workflow = TRUE)
  tree_grid <- tune_grid(
    wf |> add_model(tree_spec),
    vfold_cv(xy, v = 4),
    grid = grid_regular(cost_complexity(), levels = 10),
    control = control
  )

  metrics_plot <- collect_metrics(tree_grid) |>
    filter(.metric == "accuracy") |>
    ggplot(aes(log(cost_complexity))) +
    geom_errorbar(aes(
      ymin = mean - std_err,
      ymax = mean + std_err
    ),
    alpha = 0.5
    ) +
    geom_point(aes(y = mean), col = "#545454") +
    scale_x_reverse() +
    labs(x = "Cost Complexity", y = "CV Accuracy") +
    facet_wrap(~ .metric) +
    theme(
      strip.text = element_text(size = 16),
      axis.title = element_text(size = 18),
      axis.text = element_text(size = 16)
    )

  final_tree <- fit_best(tree_grid, metric = "accuracy")
  extract_fit_engine(final_tree) |>
    rpart.plot()
  list(fit = final_tree, grid = tree_grid, plot = metrics_plot)
}

```

```{r}
samples <- read_csv("../data/blooms.csv") |>
  mutate(class = factor(class))
xy <- select(samples, -subject)
```

```{r}
problem_rec <- recipe(class ~ ., data = xy) |>
  update_role(class, new_role = "outcome") |>
  step_normalize(all_numeric())

wf <- workflow() |>
  add_recipe(problem_rec)

tune_spec <- logistic_reg(penalty = tune(), mixture = 1) |>
  set_engine("glmnet")

lasso_result <- lasso_outputs(tune_spec, xy)
tidy(lasso_result$fit) |>
  arrange(-estimate) |>
  select(term, estimate)
lasso_result$plot
```

Here is the analogous code for a decision tree.


```{r}
tree_spec <- decision_tree(cost_complexity = tune()) |>
  set_engine("rpart") |>
  set_mode("classification")

final_tree <- tree_outputs(tree_spec, xy)
```

Let's inspect the stability of these models...

```{r}
xy_split <- initial_split(xy, prop = 0.5)

fits <- list(tree = list(), lasso = list())
split_funs <- c(training, testing)
for (i in seq_along(split_funs)) {
  fits[["lasso"]][[i]] <- wf |>
    add_model(tune_spec) |>
    finalize_workflow(select_best(lasso_result$grid, "accuracy")) |>
    fit(split_funs[[i]](xy_split))
}

coef_compare <- bind_rows(
  tidy(extract_fit_parsnip(fits[["lasso"]][[1]])),
  tidy(extract_fit_parsnip(fits[["lasso"]][[2]])),
  .id = "split"
) |>
  pivot_wider(names_from = split, values_from = estimate)

ggplot(coef_compare, aes(`1`, `2`)) +
  geom_point() +
  geom_text_repel(data = filter(coef_compare, abs(`1`) + abs(`2`) > 0.4), aes(`1`, `2`, label = term), size = 6) +
  labs(x = "Split 1", y = "Split 2", title = expression(paste(hat(beta), " ", "across splits"))) +
  theme(
    axis.title = element_text(size = 20),
    axis.text = element_text(size = 18),
    title = element_text(size = 20)
  )
```

Next approach to directly interpretability is to manually define predictive
features.

```{r}
samples_long <- samples |>
  pivot_longer(starts_with("tax"), names_to = "feature", values_to = "Freq") |>
  mutate(
    taxon = str_extract(feature, "tax[0-9]+"),
    time = as.integer(str_extract(feature, "[0-9]+$"))
    )
  
features <- samples_long |>
  group_by(taxon, subject) |>
  summarise(
    slope = coef(fastLm(Freq ~ time))[1],
    curvature = mean(diff(Freq, 2) ^ 2)
  )

xy_features <- samples_long |>
  select(subject, class, taxon) |>
  unique() |>
  left_join(features) |>
  pivot_wider(names_from = "taxon", values_from = c("slope", "curvature")) |>
  select(-subject)
```

```{r}
problem_rec <- recipe(class ~ ., data = xy_features) |>
  update_role(class, new_role = "outcome") |>
  step_normalize(all_numeric())

wf <- workflow() |>
  add_recipe(problem_rec)

tune_spec <- logistic_reg(penalty = tune(), mixture = 1) |>
  set_engine("glmnet")
tree_spec <- decision_tree(cost_complexity = tune()) |>
  set_engine("rpart") |>
  set_mode("classification")

lasso_result <- lasso_outputs(tune_spec, xy_features)
lasso_result$fit |>
  extract_fit_parsnip() |>
  tidy() |>
  summarise(sum(estimate != 0))

tree_result <- tree_outputs(tree_spec, xy_features)
tree_result$grid |>
  collect_metrics() |>
  filter(.metric == "accuracy") |>
  summarise(max(mean))
tree_result$plot
```

This does quite a bit better than ordinary glmnet. Still a little ways away from
transformers.

```{r}
# this one is cheating a little...
nearest_neighbor(neighbors = 5) |>
  set_mode("classification") |>
  error_rate(xy_features)
```

