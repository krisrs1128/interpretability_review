---
title: "Directly Interpretable Models"
output: html_document
date: "`r Sys.Date()`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

```{r}
library(RcppEigen)
library(broom)
library(glmnetUtils)
library(glue)
library(tidymodels)
library(tidyverse)
my_theme <- theme_classic() +
  theme()
theme_set(my_theme)
set.seed(20240201)
```

```{r}
error_rate <- function(spec, xy) {
  y_hat <- fit(spec, class ~ ., data = xy) |>
    predict(xy)
  errors <- bind_cols(class = xy$class, y_hat) |>
    count(class, .pred_class)
  list(errors = errors, fit = fit)
}
```

```{r}
samples <- read_csv("../data/blooms.csv") |>
  mutate(class = factor(class))
xy <- select(samples, -subject)
```

First approach to directly interpretability is to manually define predictive
features.

```{r}
features <- samples |>
  group_by(taxon, subject) |>
  summarise(
    slope = coef(fastLm(Freq ~ time))[1],
    curvature = mean(diff(Freq, 2) ^ 2)
  )
```

```{r}
xy_features <- samples |>
  select(subject, class, taxon) |>
  unique() |>
  left_join(features) |>
  pivot_wider(names_from = "taxon", values_from = c("slope", "curvature")) |>
  select(-subject)

fit_glm <- cv.glmnet(class ~ ., xy_features, family = "binomial")
y_hat <- predict(fit_glm, xy_features)
data.frame(y = xy_features$class, y_hat = ifelse(y_hat[, 1] > 0, "healthy", "disease")) |>
  count(y, y_hat)
```

This does quite a bit better than ordinary glmnet. Still a little ways away from
transformers.

```{r}
logistic_reg(penalty = 0.005) %>% 
  set_engine("glmnet") |>
  error_rate(xy)

logistic_reg(penalty = 0.005) %>% 
  set_engine("glmnet") |>
  error_rate(xy_features)

decision_tree() |>
  set_mode("classification") |>
  error_rate(xy)

decision_tree() |>
  set_mode("classification") |>
  error_rate(xy_features)

# this one is cheating a little...
nearest_neighbor(neighbors = 5) |>
  set_mode("classification") |>
  error_rate(xy_features)
```

```{r}
problem_rec <- recipe(class ~ ., data = xy) |>
  update_role(class, new_role = "outcome") |>
  step_normalize(all_numeric())
  
wf <- workflow() |>
  add_recipe(problem_rec)

tune_spec <- logistic_reg(penalty = tune(), mixture = 1) |>
  set_engine("glmnet")

lasso_grid <- tune_grid(
  wf |> add_model(tune_spec),
  vfold_cv(xy, v = 4),
  grid = grid_regular(penalty(range=c(-2.66610, -0.66610)), levels = 50)
)

p1 <- collect_metrics(lasso_grid) |>
  filter(.metric == "accuracy") |>
  ggplot(aes(log(penalty))) +
  geom_errorbar(aes(
    ymin = mean - std_err,
    ymax = mean + std_err
  ),
  alpha = 0.5
  ) +
  geom_point(aes(y = mean), col = "#545454") +
  scale_x_reverse() +
  labs(x = expression(log(lambda)), y = "CV Accuracy") +
  facet_wrap(~ .metric)

final_lasso <- finalize_workflow(
  add_model(wf, tune_spec),
  select_best(lasso_grid, "accuracy")
) |>
  fit(xy)

coefs <- tidy(final_lasso$fit$fit$fit)
p2 <- ggplot(coefs) +
  geom_hline(yintercept = 0, linewidth = 1.5) +
  geom_line(aes(log(lambda), estimate, group = term), col = "#0c0c0c") +
  labs(x = expression(log(lambda)), y = "Coefficient Estimate") +
  scale_x_reverse()

library(patchwork)

p1 / p2

tidy(final_lasso) |>
  arrange(-estimate) |>
  select(term, estimate)

```
