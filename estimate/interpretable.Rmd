---
title: "Directly Interpretable Models"
output: html_document
date: "`r Sys.Date()`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

```{r}
library(RcppEigen)
library(broom)
library(ggrepel)
library(glmnetUtils)
library(glue)
library(patchwork)
library(rpart.plot)
library(tidymodels)
library(tidyverse)
my_theme <- theme_classic() +
  theme()
theme_set(my_theme)
set.seed(20240201)
source("interpretable.R")
```

```{r}
samples <- read_csv("../data/blooms.csv") |>
  mutate(class = factor(class))
xy <- select(samples, -subject)
```

```{r}
problem_rec <- recipe(class ~ ., data = xy) |>
  update_role(class, new_role = "outcome") |>
  step_normalize(all_numeric())

wf <- workflow() |>
  add_recipe(problem_rec)

tune_spec <- logistic_reg(penalty = tune(), mixture = 1) |>
  set_engine("glmnet")

lasso_result <- lasso_outputs(tune_spec, wf, xy)
tidy(lasso_result$fit) |>
  arrange(-estimate) |>
  select(term, estimate)
lasso_result$plot
```

Here is the analogous code for a decision tree.


```{r}
tree_spec <- decision_tree(cost_complexity = tune()) |>
  set_engine("rpart") |>
  set_mode("classification")

final_tree <- tree_outputs(tree_spec, wf, xy)
```

Let's inspect the stability of these models...

```{r}
xy_split <- initial_split(xy, prop = 0.5)

fits <- list(tree = list(), lasso = list())
split_funs <- c(training, testing)
for (i in seq_along(split_funs)) {
  fits[["lasso"]][[i]] <- wf |>
    add_model(tune_spec) |>
    finalize_workflow(select_best(lasso_result$grid, "accuracy")) |>
    fit(split_funs[[i]](xy_split))
}

coef_compare <- bind_rows(
  tidy(extract_fit_parsnip(fits[["lasso"]][[1]])),
  tidy(extract_fit_parsnip(fits[["lasso"]][[2]])),
  .id = "split"
) |>
  pivot_wider(names_from = split, values_from = estimate)

ggplot(coef_compare, aes(`1`, `2`)) +
  geom_point() +
  geom_text_repel(data = filter(coef_compare, abs(`1`) + abs(`2`) > 0.4), aes(`1`, `2`, label = term), size = 6) +
  labs(x = "Split 1", y = "Split 2", title = expression(paste(hat(beta), " ", "across splits"))) +
  theme(
    axis.title = element_text(size = 20),
    axis.text = element_text(size = 18),
    title = element_text(size = 20)
  )
```

Next approach to directly interpretability is to manually define predictive
features.

```{r}
samples_long <- samples |>
  pivot_longer(starts_with("tax"), names_to = "feature", values_to = "Freq") |>
  mutate(
    taxon = str_extract(feature, "tax[0-9]+"),
    time = as.integer(str_extract(feature, "[0-9]+$"))
    )
  
features <- samples_long |>
  group_by(taxon, subject) |>
  summarise(
    slope = coef(fastLm(Freq ~ time))[1],
    curvature = mean(diff(Freq, 2) ^ 2)
  )

xy_features <- samples_long |>
  select(subject, class, taxon) |>
  unique() |>
  left_join(features) |>
  pivot_wider(names_from = "taxon", values_from = c("slope", "curvature")) |>
  select(-subject)
```

```{r}
problem_rec <- recipe(class ~ ., data = xy_features) |>
  update_role(class, new_role = "outcome") |>
  step_normalize(all_numeric())

wf <- workflow() |>
  add_recipe(problem_rec)

tune_spec <- logistic_reg(penalty = tune(), mixture = 1) |>
  set_engine("glmnet")
tree_spec <- decision_tree(cost_complexity = tune()) |>
  set_engine("rpart") |>
  set_mode("classification")

lasso_result <- lasso_outputs(tune_spec, wf, xy_features)
lasso_result$fit |>
  extract_fit_parsnip() |>
  tidy() |>
  summarise(sum(estimate != 0))

tree_result <- tree_outputs(tree_spec, wf, xy_features)
tree_result$grid |>
  collect_metrics() |>
  filter(.metric == "accuracy") |>
  summarise(max(mean))
tree_result$plot
```

This does quite a bit better than ordinary glmnet. Still a little ways away from
transformers.

```{r}
# this one is cheating a little...
nearest_neighbor(neighbors = 5) |>
  set_mode("classification") |>
  error_rate(xy_features)
```

